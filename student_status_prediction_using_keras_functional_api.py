# -*- coding: utf-8 -*-
"""Student status prediction using Keras Functional API

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15cZ113TFZdLuis5MhZuS2qaBXAmV6C8o

# Student status prediction using multiple output models and Keras Functional API

In this example I will use the student performance dataset in order to buil a model that can predict the age of the student and wheather the student in a romantic relationship or not. 
The dataset used in this example is called **[Student Performance Dataset](https://archive.ics.uci.edu/ml/datasets/Student+Performance)** from the **UCI machine learning repository**.

I will model the age of the student estimations as a regression problem and the romantic status detection as a binary classification problem.
"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import itertools

"""## Loading the Dataset to a data frame"""

#URL of the student dataset
#URL = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00320/student.zip'

# after downloading the csv file and placing it in the directory we can use the data
#student_df = pd.read_csv('student-mat.csv') this does not work without separating the data
student_df = pd.read_csv('student-mat.csv', sep=';')

#inspect some data for student "10"
print(student_df.studytime[12])
print(student_df.romantic[12])

"""###Showing the data type of the data frame"""

for name, dtype in student_df.dtypes.iteritems():
    print(name, dtype)

"""## The data frame"""

student_df.head()

"""###Before makig changes on the dataframe lets copy it and perform the changes on the copy."""

df=student_df.copy()

"""#**Catagorical encoding**
We need to transform the data that has the type "object" in the data frame into float32
"""

from sklearn.preprocessing import OrdinalEncoder

ord_enc = OrdinalEncoder()
df["famsize1"] = ord_enc.fit_transform(df[["famsize"]])
df[["famsize", "famsize1"]].head(11)

"""###Repcing"""

df.pop("famsize")

"""###Repeating the process to the other colomns
First the encoding process
"""

df["Mjob1"] = ord_enc.fit_transform(df[["Mjob"]])
df["school1"] = ord_enc.fit_transform(df[["school"]])
df["sex1"] = ord_enc.fit_transform(df[["sex"]])
df["address1"] = ord_enc.fit_transform(df[["address"]])
df["Pstatus1"] = ord_enc.fit_transform(df[["Pstatus"]])
df["Fjob1"] = ord_enc.fit_transform(df[["Fjob"]])
df["reason1"] = ord_enc.fit_transform(df[["reason"]])
df["guardian1"] = ord_enc.fit_transform(df[["guardian"]])
df["schoolsup1"] = ord_enc.fit_transform(df[["schoolsup"]])
df["famsup1"] = ord_enc.fit_transform(df[["famsup"]])
df["paid1"] = ord_enc.fit_transform(df[["paid"]])
df["activities1"] = ord_enc.fit_transform(df[["activities"]])
df["nursery1"] = ord_enc.fit_transform(df[["nursery"]])
df["higher1"] = ord_enc.fit_transform(df[["higher"]])
df["internet1"] = ord_enc.fit_transform(df[["internet"]])
df["romantic1"] = ord_enc.fit_transform(df[["romantic"]])

"""Then we get rid of the unencoded data"""

df.pop("Mjob")
df.pop("school")
df.pop("sex")
df.pop("address")
df.pop("Pstatus")
df.pop("Fjob")
df.pop("reason")
df.pop("guardian")
df.pop("schoolsup")
df.pop("famsup")
df.pop("paid")
df.pop("activities")
df.pop("nursery")
df.pop("higher")
df.pop("internet")
df.pop("romantic")

df

"""####This will chart the students ages."""

student_df['age'].hist(bins=20);

"""### Imbalanced data
You can see from the plot above that the student dataset is imbalanced. 
- Since there are very few observations with age equal to 20, 21 and 22, we can drop these observations from the dataset. 
- That can be done by removing data belonging to all greater than 20.
"""

#get data with students ages less than 20
df = df[(df['age'] < 20)]

# reset index and drop the old one
df = df.reset_index(drop=True)

"""####Now lets plot again to see the new dataset"""

df['age'].hist(bins=20);

"""####This will chart the students romantic status."""

df['romantic1'].hist(bins=20);

"""### Train Test Split

Next, we will split the datasets into training, test and validation datasets.
- The data frame will be split 80:20 into `train` and `test` sets.
- The resulting `train` will then be split 80:20 into `train` and `val` sets.
- The `train_test_split` parameter `test_size` takes a float value that ranges between 0. and 1, and represents the proportion of the dataset that is allocated to the test set.  The rest of the data is allocated to the training set.
"""

# split df into 80:20 train and test sets
train, test = train_test_split(df, test_size=0.2, random_state = 0)
                               
# split train into 80:20 train and val sets
train, val = train_test_split(train, test_size=0.2, random_state = 0)

"""Exploring the training stats. We will pop the labels 'romantic' and 'age' from the data as these will be used as the labels

"""

train_stats = train.describe()      #This does not work with data that cointains strings
train_stats.pop('romantic1')
train_stats.pop('age')
train_stats = train_stats.transpose()

train_stats

"""

### Getting the labels

The features and labels are currently in the same dataframe.
- We want to store the label columns `age` and `romantic` separately from the feature columns.  
- The following function, `format_output`, gets these two columns from the dataframe.
- `format_output` also formats the data into numpy arrays. 

"""

def format_output(data):
    age = data.pop('age')
    age = np.array(age)
    romantic1 = data.pop('romantic1')
    romantic1 = np.array(romantic1)
    return (age, romantic1)

# format the output of the train set
train_Y = format_output(train)

# format the output of the val set
val_Y = format_output(val)
    
# format the output of the test set
test_Y = format_output(test)

train_Y

"""After we get the labels, the `train`, `val` and `test` dataframes no longer contain the label columns, and contain just the feature columns.
- This is because you used `.pop` in the `format_output` function.
"""

train.head()

"""### Normalize the data

Next, we normalize the data, x, using the formula:
$$x_{norm} = \frac{x - \mu}{\sigma}$$
"""

def norm(x):
    return (x - train_stats['mean']) / train_stats['std']

# normalize the train set
norm_train_X = norm(train)
    
# normalize the val set
norm_val_X = norm(val)
    
# normalize the test set
norm_test_X = norm(test)

"""## Define the Model

Here we difine the model using the functional API. The base model will be 2 `Dense` layers of 128 neurons each, and have the `'relu'` activation.
- Check out the documentation for [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)
"""

def base_model(inputs):
    
    # connect a Dense layer with 128 neurons and a relu activation
    x = Dense(units='128', activation='relu')(inputs)
    
    # connect another Dense layer with 128 neurons and a relu activation
    x = Dense(units='128', activation='relu')(x) 
    return x

"""### Define output layers of the model

Here we add output layers to the base model. 
- The model will need two outputs.

One output layer will predict the student age, which is a numeric value.
- Define a `Dense` layer with 1 neuron.
- Since this is a regression output, the activation can be left as its default value `None`.

The other output layer will predict the student romantic status, which is either in a romantic relationship `1` or not single `0` (white).
- We define a `Dense` layer with 1 neuron.
- Since there are two possible categories, you can use a sigmoid activation for binary classification.

"""

def final_model(inputs):
    
    # get the base model
    x = base_model(inputs)

    # connect the output Dense layer for regression
    student_age = Dense(units='1', name='student_age')(x)

    # connect the output Dense layer for classification. this will use a sigmoid activation.
    student_romantic = Dense(units='1', activation='sigmoid', name='student_romantic')(x)

    # define the model using the input and output layers
    model = Model(inputs=inputs, outputs= [student_age, student_romantic])

    return model

"""## Compiling the Model

Next, we compile the model. When setting the loss parameter of `model.compile`, we're setting the loss for each of the two outputs (student age and student romantic status).


- Student romantic status: Since you will be performing binary classification on wine type, you should use the binary crossentropy loss function for it..  
  
- Student age: since this is a regression output, use the mean squared error.
 


"""

inputs = tf.keras.layers.Input(shape=(31,))#inputs = tf.keras.layers.Input(shape=(11,))
rms = tf.keras.optimizers.RMSprop(lr=0.0001)
model = final_model(inputs)

model.compile(optimizer=rms, 
              loss = {'student_romantic' : 'binary_crossentropy',
                      'student_age' :'mean_squared_error' 
                     },
              metrics = {'student_romantic' : 'accuracy',
                         'student_age': tf.keras.metrics.RootMeanSquaredError()
                       }
             )

#This is to see what the model expect in order to deal with errors
[print(i.shape, i.dtype) for i in model.inputs]
[print(o.shape, o.dtype) for o in model.outputs]
[print(l.name, l.input_shape, l.dtype) for l in model.layers]

"""This could be used if we want to convert other data typs into float32"""

# train_Y = np.asarray(train_Y).astype(np.float32)
# test_Y = np.asarray(test_Y).astype(np.float32)
# norm_train_X = np.asarray(norm_train_X).astype(np.float32)
# norm_test_X = np.asarray(norm_test_X).astype(np.float32)

type_test=np.array(train)
type_test.dtype

"""## Training the Model

Using the normalized data
"""

history = model.fit(norm_train_X ,train_Y,# norm_train_X instead of train
                    epochs = 180, validation_data=(norm_test_X , test_Y)) # norm_test_X instead of test

# Gather the training metrics
loss, student_age_loss, student_romantic_loss, student_age_rmse, student_romantic_accuracy = model.evaluate(x=norm_val_X, y=val_Y)

print()
print(f'loss: {loss}')
print(f'student_age_loss: {student_age_loss}')
print(f'student_romantic_loss: {student_romantic_loss}')
print(f'student_age_rmse: {student_age_rmse}')
print(f'student_romantic_accuracy: {student_romantic_accuracy}')

"""## Analyze the Model Performance
The model has two outputs. The output at index 0 is student age and index 1 is student romantic status

"""

predictions = model.predict(norm_test_X)
age_pred = predictions[0]
romantic_pred = predictions[1]

print(age_pred[0])
print(romantic_pred[0])

print(type_pred[9])

"""### Plot Utilities

We define a few utilities to visualize the model performance.
"""

def plot_metrics(metric_name, title, ylim=5):
    plt.title(title)
    plt.ylim(0,ylim)
    plt.plot(history.history[metric_name],color='blue',label=metric_name)
    plt.plot(history.history['val_' + metric_name],color='green',label='val_' + metric_name)

def plot_confusion_matrix(y_true, y_pred, title='', labels=[0,1]):
    cm = confusion_matrix(y_true, y_pred)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    cax = ax.matshow(cm)
    plt.title('Confusion matrix of the classifier')
    fig.colorbar(cax)
    ax.set_xticklabels([''] + labels)
    ax.set_yticklabels([''] + labels)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
          plt.text(j, i, format(cm[i, j], fmt),
                  horizontalalignment="center",
                  color="black" if cm[i, j] > thresh else "white")
    plt.show()

def plot_diff(y_true, y_pred, title = '' ):
    plt.scatter(y_true, y_pred)
    plt.title(title)
    plt.xlabel('True Values')
    plt.ylabel('Predictions')
    plt.axis('equal')
    plt.axis('square')
    plt.plot([-100, 100], [-100, 100])
    return plt

"""### Plots for Metrics"""

plot_metrics('student_age_loss', 'RMSE', ylim=10)

plot_metrics('student_romantic_loss', 'student_romantic_loss', ylim=0.8)

"""### Plots for Confusion Matrix

Plot the confusion matrices for student romantic status. We can see that the model performs well for prediction of romantic from the confusion matrix and the loss metrics.
"""

plot_confusion_matrix(test_Y[1], np.round(type_pred), title='Wine Type', labels = [0, 1])

scatter_plot = plot_diff(test_Y[0], quality_pred, title='Type')